"""
==============================================================================
FILE: services/analysis/prediction_service.py
ROLE: Predictive Modeling Engine
PURPOSE:
    Train and execute Machine Learning models to forecast market direction.
    Currently uses XGBoost Classifier.
    
    - Target: Next period Returns > 0 (Binary Classification)
    - Features: Generated by FeatureEngineeringService
    
ROADMAP: Phase 14 - Predictive Modeling
==============================================================================
"""

import logging
import os
import joblib
import pandas as pd
import numpy as np
from typing import Dict, Any, Optional, Tuple

# Conditional imports to handle missing dependencies during initial setup
try:
    from xgboost import XGBClassifier
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score, precision_score
except ImportError:
    XGBClassifier = None
    train_test_split = None
    accuracy_score = None
    precision_score = None

from services.analysis.feature_service import get_feature_service

logger = logging.getLogger(__name__)

MODELS_DIR = os.path.join(os.getcwd(), "data", "models")
os.makedirs(MODELS_DIR, exist_ok=True)

class PredictionService:
    def __init__(self):
        self.model = None
        self.feature_service = get_feature_service()
        self.model_path = os.path.join(MODELS_DIR, "xgb_direction_model.pkl")
        self._load_model()

    def train_model(self, df: pd.DataFrame, target_col: str = 'returns', test_size: float = 0.2) -> Dict[str, float]:
        """
        Train a new XGBoost model on the provided DataFrame.
        """
        if XGBClassifier is None:
            raise ImportError("XGBoost/Scikit-Learn not installed.")
            
        logger.info(f"Starting training on {len(df)} rows...")
        
        # 1. Generate Features
        df_features = self.feature_service.generate_features(df)
        
        # 2. Create Target (1 if Return > 0, else 0) - Shifted by -1 to predict NEXT candle
        # We want features at T to predict Returns at T+1
        df_features['target'] = (df_features[target_col].shift(-1) > 0).astype(int)
        
        # Drop NaN (created by lags and target shift)
        df_clean = df_features.dropna()
        
        if df_clean.empty:
            raise ValueError("Not enough data to train after feature engineering.")
            
        # 3. Split Features (X) and Target (y)
        # Exclude non-feature columns
        exclude_cols = ['open', 'high', 'low', 'close', 'volume', 'target', 'timestamp', 'returns', 'log_returns']
        feature_cols = [c for c in df_clean.columns if c not in exclude_cols]
        
        X = df_clean[feature_cols]
        y = df_clean['target']
        
        # 4. Train/Test Split
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, shuffle=False)
        
        # 5. Initialize and Train XGBoost
        self.model = XGBClassifier(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=5,
            eval_metric='logloss',
            use_label_encoder=False
        )
        self.model.fit(X_train, y_train)
        
        # 6. Evaluate
        preds = self.model.predict(X_test)
        acc = accuracy_score(y_test, preds)
        prec = precision_score(y_test, preds, zero_division=0)
        
        metrics = {"accuracy": float(acc), "precision": float(prec)}
        logger.info(f"Training complete. Metrics: {metrics}")
        
        # 7. Save
        self.save_model()
        
        return metrics

    def predict_direction(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        Predict the probability of an UP move for the latest data point.
        """
        if self.model is None:
            return {"error": "Model not trained or loaded"}
            
        # Generate features for the passed dataframe
        df_features = self.feature_service.generate_features(df)
        
        if df_features.empty:
            return {"error": "No data to predict"}
            
        # Extract latest feature vector
        latest_row = df_features.iloc[[-1]] # Keep as DataFrame
        
        # Ensure we use same columns as training (naive check, improved in robustness later)
        # For now, we rely on FeatureService determinism.
        exclude_cols = ['open', 'high', 'low', 'close', 'volume', 'target', 'timestamp', 'returns', 'log_returns']
        feature_cols = [c for c in latest_row.columns if c not in exclude_cols and c in self.model.get_booster().feature_names]
        
        # If columns mismatch, we might have an issue. XGBoost is sensitive to feature order/names.
        # Ideally we save feature names with the model.
        
        X_new = latest_row[feature_cols]
        
        # Predict Probability
        prob_up = float(self.model.predict_proba(X_new)[0][1]) # Probability of class 1
        prediction = "UP" if prob_up > 0.5 else "DOWN"
        
        return {
            "prediction": prediction,
            "probability_up": prob_up,
            "features_used": feature_cols,
            "timestamp": str(pd.Timestamp.now())
        }

    def save_model(self):
        if self.model:
            joblib.dump(self.model, self.model_path)
            logger.info(f"Model saved to {self.model_path}")

    def _load_model(self):
        if os.path.exists(self.model_path):
            try:
                self.model = joblib.load(self.model_path)
                logger.info(f"Model loaded from {self.model_path}")
            except Exception as e:
                logger.error(f"Failed to load model: {e}")

# Singleton
_instance = None

def get_prediction_service() -> PredictionService:
    global _instance
    if _instance is None:
        _instance = PredictionService()
    return _instance
